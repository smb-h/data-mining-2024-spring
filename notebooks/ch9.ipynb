{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1. Fuzzy clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy clustering is an extension of traditional clustering methods that allows data points to belong to multiple clusters simultaneously, each with a degree of membership. Unlike hard clustering, where a point exclusively belongs to a single cluster, fuzzy clustering introduces a degree of uncertainty. The primary model used for fuzzy clustering is the fuzzy c-means (FCM) algorithm.\n",
    "\n",
    "In FCM, each data point has membership values across all clusters, represented as a fuzzy membership matrix. These membership values range between 0 and 1, indicating the likelihood of a data point belonging to a specific cluster. Fuzzy clustering is particularly useful when data points exhibit partial membership to multiple clusters.\n",
    "\n",
    "#### A real-world example of practical use in Python for fuzzy clustering using the Fuzzy C-Means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Apply Fuzzy C-Means clustering\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "def fuzzy_c_means(X, n_clusters, m=2, max_iter=100, tol=1e-4):\n",
    "    # Initialize cluster centers using K-Medoids\n",
    "    kmedoids = KMedoids(n_clusters=n_clusters)\n",
    "    kmedoids.fit(X)\n",
    "    centers = kmedoids.cluster_centers_\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Calculate distances and update membership matrix\n",
    "        distances = pairwise_distances_argmin_min(X, centers)[1]\n",
    "        membership_matrix = 1 / distances[:, None] ** (2 / (m - 1))\n",
    "        membership_matrix = membership_matrix / np.sum(membership_matrix, axis=1)[:, None]\n",
    "\n",
    "        # Update cluster centers\n",
    "        new_centers = np.dot(membership_matrix.T, X) / np.sum(membership_matrix, axis=0)[:, None]\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(new_centers - centers) < tol:\n",
    "            break\n",
    "\n",
    "        centers = new_centers\n",
    "\n",
    "    # Assign final clusters based on maximum membership\n",
    "    labels = np.argmax(membership_matrix, axis=1)\n",
    "    return labels, centers\n",
    "\n",
    "# Apply Fuzzy C-Means clustering\n",
    "n_clusters = 3\n",
    "fuzzy_labels, fuzzy_centers = fuzzy_c_means(X, n_clusters)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=fuzzy_labels, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(fuzzy_centers[:, 0], fuzzy_centers[:, 1], c='red', marker='X', s=200, label='Cluster Centers')\n",
    "plt.title('Fuzzy C-Means Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data using make_blobs from scikit-learn and then apply the Fuzzy C-Means algorithm using a custom implementation. The resulting fuzzy clusters and cluster centers are visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2. Probabilistic model-based clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic model-based clustering involves the use of statistical models to describe the underlying distribution of data and identify clusters based on the parameters of these models. One common approach is the Gaussian Mixture Model (GMM), where data points are assumed to be generated from a mixture of several Gaussian distributions.\n",
    "\n",
    "In GMM-based clustering, each cluster is associated with a Gaussian distribution characterized by its mean, covariance matrix, and weight. The model allows for more flexibility in capturing complex cluster shapes and provides a probability distribution over cluster assignments. The Expectation-Maximization (EM) algorithm is often employed for parameter estimation in GMMs.\n",
    "\n",
    "#### A real-world example of practical use in Python for probabilistic model-based clustering using the Gaussian Mixture Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Apply Gaussian Mixture Model clustering\n",
    "n_components = 3\n",
    "gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "gmm.fit(X)\n",
    "probabilities = gmm.predict_proba(X)  # Probabilities of each sample belonging to each cluster\n",
    "labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='red', marker='X', s=200, label='Cluster Centers')\n",
    "plt.title('Gaussian Mixture Model Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data using make_blobs from scikit-learn and then apply the Gaussian Mixture Model for clustering. The resulting probabilistic model-based clusters and cluster centers are visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.3. Expectation-maximization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Expectation-Maximization (EM) algorithm is a general framework for finding maximum likelihood estimates of parameters in probabilistic models with latent variables. In the context of probabilistic model-based clustering, the EM algorithm is often used for parameter estimation in Gaussian Mixture Models (GMMs).\n",
    "\n",
    "Here's a brief overview of the EM algorithm in the context of GMM-based clustering:\n",
    "\n",
    "- Expectation Step (E-step):\n",
    "    Compute the expected value of the latent variables (cluster assignments) given the observed data and the current parameter estimates.\n",
    "\n",
    "- Maximization Step (M-step):\n",
    "    Update the parameters (mean, covariance, and weights) of the Gaussian distributions based on the observed data and the expected values obtained in the E-step.\n",
    "\n",
    "- Iteration:\n",
    "    Repeat the E-step and M-step until convergence, where the change in the log-likelihood or parameters is below a predefined threshold.\n",
    "\n",
    "The EM algorithm iteratively refines the parameter estimates, maximizing the likelihood of the observed data given the model.\n",
    "\n",
    "#### Real-world example of practical use in Python for the Expectation-Maximization algorithm applied to Gaussian Mixture Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Apply Gaussian Mixture Model clustering with the Expectation-Maximization algorithm\n",
    "n_components = 3\n",
    "gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "gmm.fit(X)\n",
    "labels = gmm.predict(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='red', marker='X', s=200, label='Cluster Centers')\n",
    "plt.title('Gaussian Mixture Model Clustering with EM Algorithm')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data using make_blobs from scikit-learn and then apply the Gaussian Mixture Model with the EM algorithm for clustering. The resulting clusters and cluster centers are visualized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2. Axis-parallel subspace approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering high-dimensional data poses unique challenges, such as the curse of dimensionality and the presence of irrelevant or redundant features. Axis-parallel subspace approaches aim to address these challenges by identifying subspaces within the high-dimensional feature space where clusters are more apparent.\n",
    "\n",
    "Key characteristics of axis-parallel subspace approaches include:\n",
    "\n",
    "1. Feature Subset Selection:\n",
    "        These methods involve selecting a subset of features or dimensions that are most informative for clustering, discarding irrelevant or redundant dimensions.\n",
    "\n",
    "2. Axis-Parallel Clustering:\n",
    "        Clustering is performed within the selected subspace using methods that are sensitive to the geometry of the data along each axis.\n",
    "\n",
    "3. Projection and Visualization:\n",
    "        Subspace clustering often includes projecting the data onto the selected subspace for better visualization and interpretation.\n",
    "\n",
    "Common techniques for axis-parallel subspace clustering include subspace clustering based on k-means or density-based clustering within subspaces.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for axis-parallel subspace clustering using the k-means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic high-dimensional data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0, n_features=10)\n",
    "\n",
    "# Apply Principal Component Analysis (PCA) for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Apply KMeans clustering in the reduced subspace\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Plot the results in the reduced subspace\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Cluster Centers')\n",
    "plt.title('KMeans Clustering in Reduced Subspace using PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic high-dimensional data using make_blobs from scikit-learn, apply Principal Component Analysis (PCA) for dimensionality reduction, and then perform KMeans clustering in the reduced subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.3. Arbitrarily oriented subspace approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arbitrarily oriented subspace approaches address the challenges of clustering high-dimensional data by identifying subspaces that are not necessarily aligned with the axes of the original feature space. Unlike axis-parallel approaches, these methods consider subspaces with arbitrary orientations, allowing for more flexibility in capturing complex relationships within the data.\n",
    "\n",
    "Key characteristics of arbitrarily oriented subspace approaches include:\n",
    "\n",
    "- Subspace Definition:\n",
    "    These methods allow for the definition of subspaces based on linear combinations of original features, enabling the identification of clusters in non-axis-parallel subspaces.\n",
    "\n",
    "- Subspace Clustering Techniques:\n",
    "    Techniques such as subspace clustering based on affinity propagation or spectral clustering are often employed to identify clusters within the defined subspaces.\n",
    "\n",
    "- Robustness to Feature Redundancy:\n",
    "    Arbitrarily oriented subspace approaches are generally more robust to feature redundancy, making them suitable for high-dimensional datasets with correlated features.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for arbitrarily oriented subspace clustering using the Spectral Clustering algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic high-dimensional data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0, n_features=10)\n",
    "\n",
    "# Apply Principal Component Analysis (PCA) for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Apply Spectral Clustering in the reduced subspace\n",
    "spectral_clustering = SpectralClustering(n_clusters=3, affinity='nearest_neighbors', random_state=42)\n",
    "labels = spectral_clustering.fit_predict(X)\n",
    "\n",
    "# Plot the results in the reduced subspace\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Spectral Clustering in Reduced Subspace using PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic high-dimensional data using make_blobs from scikit-learn, apply Principal Component Analysis (PCA) for dimensionality reduction, and then perform Spectral Clustering in the reduced subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2. Types of biclusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biclustering is a technique that simultaneously clusters both rows and columns of a dataset, identifying submatrices that exhibit cohesive patterns. There are several types of biclusters based on the nature of the patterns they capture:\n",
    "\n",
    "- Constant Value Biclusters:\n",
    "    Biclusters where all the elements have a constant value, indicating uniformity within the identified submatrix.\n",
    "\n",
    "- Shift Biclusters:\n",
    "    Biclusters where the values within the submatrix exhibit a constant shift, preserving the overall pattern but allowing for variation.\n",
    "\n",
    "- Additive Biclusters:\n",
    "    Biclusters where the values can be modeled as a sum of a constant and a shift, capturing both constant and shifting patterns simultaneously.\n",
    "\n",
    "- Multiplicative Biclusters:\n",
    "    Biclusters where the values can be modeled as a product of a constant and a shift, capturing both constant and scaling patterns.\n",
    "\n",
    "Understanding the type of bicluster is essential for interpreting the biological or domain-specific meaning behind the patterns identified in the data.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for biclustering using the Bicluster class from the sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_checkerboard\n",
    "from sklearn.cluster import SpectralBiclustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data with a checkerboard pattern\n",
    "data, _ = make_checkerboard(shape=(300, 300), n_clusters=(3, 3), noise=10, random_state=42)\n",
    "\n",
    "# Apply Spectral Biclustering\n",
    "model = SpectralBiclustering(n_clusters=(3, 3), random_state=42)\n",
    "model.fit(data)\n",
    "\n",
    "# Get row and column indices of the biclusters\n",
    "rows, cols = model.row_labels_, model.column_labels_\n",
    "\n",
    "# Plot the original and biclustered data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original Data')\n",
    "plt.imshow(data, cmap='viridis', aspect='auto')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Biclustered Data')\n",
    "plt.imshow(data[np.argsort(rows)], cmap='viridis', aspect='auto')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with a checkerboard pattern using make_checkerboard from scikit-learn and then apply Spectral Biclustering. The original and biclustered data are visualized side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.3. Biclustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biclustering methods are designed to discover patterns in datasets where subsets of both rows and columns exhibit similar behavior. These methods provide a powerful tool for uncovering complex structures in various types of data. Some common biclustering algorithms include:\n",
    "\n",
    "- Spectral Biclustering:\n",
    "    This method uses eigenvalue decomposition to identify biclusters in the data. It considers both row and column relationships and is particularly effective for discovering additive biclusters.\n",
    "\n",
    "- Plaid Model Biclustering:\n",
    "    The Plaid Model represents the data as a superposition of additive biclusters. Biclusters are found by iteratively fitting row and column patterns.\n",
    "\n",
    "- Bimax Biclustering:\n",
    "    Bimax is a binary matrix factorization method that aims to find submatrices containing only 0s and 1s. It is suitable for binary data.\n",
    "\n",
    "- Order-Preserving Biclustering:\n",
    "    This method identifies biclusters by preserving the order of data points within rows and columns, capturing patterns based on monotonic relationships.\n",
    "\n",
    "Each biclustering method is designed to address specific characteristics of the data and the types of patterns one expects to find.\n",
    "\n",
    "#### Real-world example of practical use in Python for biclustering using the SpectralBiclustering class from the sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_checkerboard\n",
    "from sklearn.cluster import SpectralBiclustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data with a checkerboard pattern\n",
    "data, _ = make_checkerboard(shape=(300, 300), n_clusters=(3, 3), noise=10, random_state=42)\n",
    "\n",
    "# Apply Spectral Biclustering\n",
    "model = SpectralBiclustering(n_clusters=(3, 3), random_state=42)\n",
    "model.fit(data)\n",
    "\n",
    "# Get row and column indices of the biclusters\n",
    "rows, cols = model.row_labels_, model.column_labels_\n",
    "\n",
    "# Plot the original and biclustered data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original Data')\n",
    "plt.imshow(data, cmap='viridis', aspect='auto')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Biclustered Data')\n",
    "plt.imshow(data[np.argsort(rows)], cmap='viridis', aspect='auto')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with a checkerboard pattern using make_checkerboard from scikit-learn and then apply Spectral Biclustering. The original and biclustered data are visualized side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.1. Linear dimensionality reduction methods for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear dimensionality reduction methods aim to reduce the number of features in a dataset while preserving essential information. These techniques are particularly useful for clustering high-dimensional data, as they can enhance the efficiency and interpretability of clustering algorithms. Some common linear dimensionality reduction methods for clustering include:\n",
    "\n",
    "1. Principal Component Analysis (PCA):\n",
    "    PCA identifies a set of orthogonal axes (principal components) that capture the maximum variance in the data. It is effective for reducing dimensionality while preserving the overall structure.\n",
    "\n",
    "2. Linear Discriminant Analysis (LDA):\n",
    "    LDA aims to maximize the separation between different classes in the data. It is particularly useful for clustering when class information is available.\n",
    "\n",
    "3. Non-Negative Matrix Factorization (NMF):\n",
    "    NMF factorizes the data matrix into non-negative matrices, representing parts-based, interpretable features. It is suitable for data with non-negative values.\n",
    "\n",
    "4. Independent Component Analysis (ICA):\n",
    "    ICA seeks to find independent components in the data. It is useful for separating mixed signals and identifying underlying sources.\n",
    "\n",
    "These linear dimensionality reduction methods can be applied before clustering to reduce noise, improve computational efficiency, and enhance the quality of cluster assignments.\n",
    "\n",
    "#### A real-world example of practical use in Python for linear dimensionality reduction using PCA for clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic high-dimensional data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0, n_features=10)\n",
    "\n",
    "# Apply Principal Component Analysis (PCA) for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Apply KMeans clustering in the reduced subspace\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Plot the results in the reduced subspace\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Cluster Centers')\n",
    "plt.title('KMeans Clustering after PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic high-dimensional data using make_blobs from scikit-learn, apply Principal Component Analysis (PCA) for dimensionality reduction, and then perform KMeans clustering in the reduced subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.2. Nonnegative matrix factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonnegative Matrix Factorization (NMF) is a dimensionality reduction technique that factorizes a nonnegative data matrix into two lower-dimensional matrices, where all elements are nonnegative. One matrix represents the basis vectors (features), and the other represents the coefficients for each data point in terms of these basis vectors. NMF is particularly useful when the data has nonnegative values and when parts-based, interpretable features are desired.\n",
    "\n",
    "#### Key points about NMF:\n",
    "\n",
    "- Nonnegativity Constraint:\n",
    "    Both the basis vectors and coefficients in NMF are constrained to be nonnegative, which makes the resulting factors additive and interpretable.\n",
    "\n",
    "- Applications in Clustering:\n",
    "    NMF is often employed for clustering tasks, as it can reveal parts-based representations of the data, making it easier to interpret the underlying structure.\n",
    "\n",
    "- Suitability for Text and Image Data:\n",
    "    NMF is commonly used in natural language processing for topic modeling and in image analysis for extracting meaningful components.\n",
    "\n",
    "- Parameters:\n",
    "    The number of components in the factorization is a crucial parameter that determines the dimensionality of the reduced space.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for NMF applied to clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Load the 20 newsgroups dataset (replace this with your dataset)\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer, NMF, and KMeans\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "nmf = NMF(n_components=10, random_state=42)\n",
    "kmeans = KMeans(n_clusters=20, random_state=42)\n",
    "\n",
    "pipeline = make_pipeline(vectorizer, nmf, kmeans)\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline.fit(newsgroups.data)\n",
    "\n",
    "# Display the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_words = np.argsort(nmf.components_, axis=1)[:, :-11:-1]\n",
    "\n",
    "for i, topic in enumerate(top_words):\n",
    "    words = [feature_names[j] for j in topic]\n",
    "    print(f\"Topic {i + 1}: {', '.join(words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use NMF for dimensionality reduction in a pipeline that includes text vectorization (TfidfVectorizer), NMF, and KMeans clustering. This pipeline is applied to the 20 newsgroups dataset, and the top words for each topic are displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.3. Spectral clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral Clustering is a technique that leverages the spectral properties of the data to perform clustering. It is particularly effective when dealing with non-convex clusters and complex geometric structures. Spectral Clustering involves the following steps:\n",
    "\n",
    "1. Graph Construction:\n",
    "        Construct an affinity matrix representing the pairwise similarity between data points. Common affinity measures include Gaussian similarity or nearest neighbors.\n",
    "\n",
    "2. Graph Transformation:\n",
    "        Transform the affinity matrix into a graph Laplacian matrix. The Laplacian matrix captures the connectivity and relationships between data points.\n",
    "\n",
    "3. Eigenvalue Decomposition:\n",
    "        Compute the eigenvalues and eigenvectors of the Laplacian matrix. The eigenvectors corresponding to the smallest eigenvalues encode the cluster structure in the data.\n",
    "\n",
    "4. Dimensionality Reduction:\n",
    "        Use the eigenvectors corresponding to the smallest eigenvalues as a reduced representation of the data.\n",
    "\n",
    "5. Clustering:\n",
    "        Apply a standard clustering algorithm (e.g., KMeans) to the reduced representation to obtain the final clusters.\n",
    "\n",
    "Spectral Clustering is powerful for identifying clusters with intricate shapes and is robust to noise and outliers.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for Spectral Clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data with non-convex clusters\n",
    "X, _ = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Apply Spectral Clustering\n",
    "spectral = SpectralClustering(n_clusters=4, random_state=42, affinity='nearest_neighbors')\n",
    "labels = spectral.fit_predict(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Spectral Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with non-convex clusters using make_blobs from scikit-learn and then apply Spectral Clustering. The resulting clusters are visualized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.2. Similarity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of clustering graph and network data, similarity measures play a crucial role in quantifying the degree of resemblance or relatedness between nodes. These measures help in constructing affinity matrices, which are essential for various clustering algorithms. Some common similarity measures for graph and network data include:\n",
    "\n",
    "- Jaccard Similarity:\n",
    "        Measures the similarity between two sets by dividing the size of their intersection by the size of their union.\n",
    "\n",
    "- Cosine Similarity:\n",
    "        Measures the cosine of the angle between two vectors. In the context of graphs, vectors represent node neighborhoods or attribute vectors.\n",
    "\n",
    "- Graph Edit Distance:\n",
    "        Quantifies the dissimilarity between two graphs by measuring the minimum cost of transforming one graph into another through a series of edit operations (e.g., node deletions, insertions, or edge modifications).\n",
    "\n",
    "- Node and Edge Overlap:\n",
    "        Measures the overlap of nodes or edges between two graphs, providing a simple measure of similarity based on shared elements.\n",
    "\n",
    "- Pearson Correlation Coefficient:\n",
    "        Measures the linear correlation between two variables. In the context of graphs, it can be used to measure the correlation between node attributes.\n",
    "\n",
    "Selecting an appropriate similarity measure depends on the characteristics of the graph data and the specific clustering task at hand.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for calculating Jaccard Similarity on graph data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "# Create a graph (replace this with your graph data)\n",
    "G1 = nx.Graph([(1, 2), (2, 3), (3, 4)])\n",
    "G2 = nx.Graph([(1, 2), (2, 3), (3, 4), (4, 5)])\n",
    "\n",
    "# Convert graph data to adjacency matrices\n",
    "adj_matrix1 = nx.to_numpy_array(G1)\n",
    "adj_matrix2 = nx.to_numpy_array(G2)\n",
    "\n",
    "# Flatten the adjacency matrices for Jaccard Similarity calculation\n",
    "flat_matrix1 = adj_matrix1.flatten()\n",
    "flat_matrix2 = adj_matrix2.flatten()\n",
    "\n",
    "# Calculate Jaccard Similarity\n",
    "jaccard_similarity = jaccard_similarity_score(flat_matrix1, flat_matrix2)\n",
    "\n",
    "print(f\"Jaccard Similarity between G1 and G2: {jaccard_similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we create two simple graphs using NetworkX, convert them to adjacency matrices, flatten the matrices, and then calculate the Jaccard Similarity between them using scikit-learn's jaccard_similarity_score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.3. Graph clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph clustering methods aim to identify groups of nodes within a graph that are more densely connected to each other than to nodes outside the group. Clustering in graphs is particularly useful for uncovering community structures, detecting functional modules, or identifying cohesive groups of related entities. Some common graph clustering methods include:\n",
    "\n",
    "1. Louvain Modularity Optimization:\n",
    "        Maximizes the modularity of a partition, which measures the density of edges within clusters compared to the density expected in a random network.\n",
    "\n",
    "2. Spectral Clustering:\n",
    "        Applies spectral decomposition to the Laplacian matrix of the graph to identify clusters based on the eigenvectors corresponding to the smallest eigenvalues.\n",
    "\n",
    "3. Walktrap:\n",
    "        Measures the similarity between nodes based on random walks in the graph, with nodes that are frequently visited together considered more similar.\n",
    "\n",
    "4. Infomap:\n",
    "        Models the network as a flow of information and seeks to find partitions that compress the description length of the information flow.\n",
    "\n",
    "5. Kernighan-Lin Bisection:\n",
    "        A heuristic method that recursively partitions the graph into two halves by optimizing a cost function related to edge cuts.\n",
    "\n",
    "These methods can be applied to various types of graphs, including social networks, biological networks, citation networks, and more.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for graph clustering using Louvain Modularity Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a graph (replace this with your graph data)\n",
    "G = nx.karate_club_graph()\n",
    "\n",
    "# Apply Louvain Modularity Optimization\n",
    "partition = community.best_partition(G)\n",
    "\n",
    "# Visualize the results\n",
    "pos = nx.spring_layout(G)\n",
    "colors = [partition[node] for node in G.nodes]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "nx.draw(G, pos, node_color=colors, with_labels=True, cmap=plt.cm.viridis)\n",
    "plt.title('Graph Clustering using Louvain Modularity Optimization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Zachary's Karate Club graph as an example graph from NetworkX and apply Louvain Modularity Optimization for graph clustering. The resulting clusters are visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.1. Semisupervised clustering on partially labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi-supervised clustering deals with scenarios where only a subset of the data points in a dataset are labeled. This approach incorporates both labeled and unlabeled data to enhance the clustering process. In partially labeled data, only a fraction of the data points have ground truth labels, while the rest are unlabeled. Semi-supervised clustering methods aim to leverage the available labeled information to guide the clustering of the entire dataset.\n",
    "\n",
    "Key points about semi-supervised clustering on partially labeled data:\n",
    "\n",
    "- Incorporating Labeled Information:\n",
    "        Semi-supervised clustering integrates labeled instances into the clustering process, enhancing the accuracy of cluster assignments.\n",
    "\n",
    "- Soft Constraints:\n",
    "        Algorithms often use soft constraints that encourage similar behavior between labeled and unlabeled data points, allowing the model to learn from both sources.\n",
    "\n",
    "- Hybrid Approaches:\n",
    "        Semi-supervised clustering methods can combine traditional clustering techniques with supervised learning algorithms to create a hybrid model.\n",
    "\n",
    "- Handling Noisy Labels:\n",
    "        These methods are designed to handle noise and errors in the labeled data, providing robust clustering in the presence of imperfect labels.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for semi-supervised clustering on partially labeled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data with three clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Label a small fraction of the data (simulate partially labeled data)\n",
    "rng = np.random.RandomState(42)\n",
    "random_unlabeled_points = rng.rand(len(y)) < 0.9\n",
    "y[random_unlabeled_points] = -1  # Assign -1 to indicate unlabeled points\n",
    "\n",
    "# Apply KMeans to the entire dataset (including unlabeled points)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Apply LabelPropagation using the partially labeled data\n",
    "label_propagation = LabelPropagation(kernel='knn', n_neighbors=10)\n",
    "label_propagation.fit(X, y)\n",
    "lp_labels = label_propagation.predict(X)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('KMeans Clustering')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=lp_labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('LabelPropagation Clustering')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with three clusters using make_blobs from scikit-learn. We label a small fraction of the data and use KMeans for clustering the entire dataset. Additionally, we apply LabelPropagation for semi-supervised clustering using the partially labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.2. Semisupervised clustering on pairwise constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi-supervised clustering using pairwise constraints involves providing the algorithm with pairwise relationships or constraints between data points. These constraints can be in the form of must-link constraints (indicating that two points must belong to the same cluster) or cannot-link constraints (indicating that two points cannot belong to the same cluster). By incorporating such pairwise information, the clustering algorithm guides the grouping of data points, taking into account the specified relationships.\n",
    "\n",
    "Key points about semi-supervised clustering on pairwise constraints:\n",
    "\n",
    "- Must-Link and Cannot-Link Constraints:\n",
    "        Must-link constraints specify that two data points should belong to the same cluster, while cannot-link constraints specify that they should be in different clusters.\n",
    "\n",
    "- Constraint Incorporation:\n",
    "        Algorithms leverage these pairwise constraints to guide the clustering process, adjusting cluster assignments based on the specified relationships.\n",
    "\n",
    "- Handling Noisy Constraints:\n",
    "        Semi-supervised clustering methods on pairwise constraints are designed to handle noisy or conflicting constraints, providing robustness to imperfect information.\n",
    "\n",
    "- Improving Clustering Accuracy:\n",
    "        By incorporating pairwise constraints, the algorithm aims to improve the accuracy of cluster assignments, especially when dealing with complex or overlapping structures.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for semi-supervised clustering on pairwise constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data with three clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Introduce pairwise constraints (simulating noisy constraints)\n",
    "rng = np.random.RandomState(42)\n",
    "must_link = list(combinations(range(len(y)), 2))\n",
    "cannot_link = list(combinations(rng.choice(np.where(y == 0)[0], 2), 2))\n",
    "\n",
    "# Create a pairwise affinity matrix\n",
    "affinity_matrix = np.ones((len(y), len(y)))\n",
    "for i, j in must_link:\n",
    "    affinity_matrix[i, j] = 1\n",
    "    affinity_matrix[j, i] = 1\n",
    "\n",
    "for i, j in cannot_link:\n",
    "    affinity_matrix[i, j] = 0\n",
    "    affinity_matrix[j, i] = 0\n",
    "\n",
    "# Apply LabelPropagation using pairwise constraints\n",
    "label_propagation = LabelPropagation(kernel='knn', n_neighbors=10)\n",
    "label_propagation.fit(X, y, affinity_matrix=affinity_matrix)\n",
    "lp_labels = label_propagation.predict(X)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "plt.title('True Clustering')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=lp_labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('LabelPropagation Clustering with Pairwise Constraints')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with three clusters using make_blobs from scikit-learn. We introduce pairwise constraints (must-link and cannot-link) and apply LabelPropagation for semi-supervised clustering using these constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.3. Other types of background knowledge for semisupervised clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In semi-supervised clustering, leveraging additional background knowledge beyond pairwise constraints or labeled instances can further enhance the clustering process. Other types of background knowledge may include:\n",
    "\n",
    "- Attribute Constraints:\n",
    "        Constraints on the attributes or features of data points can guide the clustering algorithm. For example, specifying that certain features should be similar or dissimilar across clusters.\n",
    "\n",
    "- Hierarchy Information:\n",
    "        Information about hierarchical relationships between clusters or data points can be incorporated. This is particularly useful when the data exhibits a nested or hierarchical structure.\n",
    "\n",
    "- Prior Probabilities:\n",
    "        Providing prior probabilities for data points belonging to specific clusters can guide the algorithm in situations where certain clusters are expected to be more prevalent.\n",
    "\n",
    "- Spatial Constraints:\n",
    "        Incorporating constraints based on the spatial arrangement of data points can be useful, especially in applications where the proximity or arrangement holds meaningful information.\n",
    "\n",
    "- Temporal Information:\n",
    "        For time-series data, temporal constraints or information about the temporal ordering of data points can be valuable in guiding the clustering process.\n",
    "\n",
    "The incorporation of diverse types of background knowledge allows semi-supervised clustering algorithms to adapt to various data characteristics and application-specific requirements.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for semi-supervised clustering with attribute constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data with three clusters and additional attribute information\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "attribute_information = np.random.rand(len(y), 5)  # Simulated additional attribute information\n",
    "\n",
    "# Introduce attribute constraints (simulating constraints on attributes)\n",
    "attribute_constraints = []\n",
    "for i in range(5):\n",
    "    cluster1, cluster2 = np.random.choice(range(3), size=2, replace=False)\n",
    "    attribute_constraints.append((cluster1, cluster2, i))  # Attribute i should be similar/dissimilar in clusters\n",
    "\n",
    "# Apply Spectral Clustering without using attribute constraints\n",
    "spectral = SpectralClustering(n_clusters=3, random_state=42)\n",
    "spectral_labels = spectral.fit_predict(X)\n",
    "\n",
    "# Apply LabelPropagation using attribute constraints\n",
    "label_propagation = LabelPropagation(kernel='knn', n_neighbors=10)\n",
    "label_propagation.fit(X, y, node_features=attribute_information, attribute_constraints=attribute_constraints)\n",
    "lp_labels = label_propagation.predict(X)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=spectral_labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Spectral Clustering without Attribute Constraints')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=lp_labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('LabelPropagation Clustering with Attribute Constraints')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with three clusters and additional attribute information. We introduce attribute constraints and compare the results of Spectral Clustering without using attribute constraints and LabelPropagation with attribute constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining_2024_spring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
